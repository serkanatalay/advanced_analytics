{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, StopWordsRemover, CountVectorizer, IDF, RegexTokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = spark.read.json(\"C:/Users/Serkan/OneDrive - KU Leuven/LEUVEN/KUL_STAT/Semester2/Advanced Analytics in a Big Data World/assignments/three.new/savedata-*\")\n",
    "tweets2 = spark.read.json(\"C:/Users/Serkan/OneDrive - KU Leuven/LEUVEN/KUL_STAT/Semester2/Advanced Analytics in a Big Data World/assignments/tweets/tweets-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets1.union(tweets2).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         label|count|\n",
      "+--------------+-----+\n",
      "|      #vaccine| 3877|\n",
      "|        #covid| 3678|\n",
      "|        #china| 3119|\n",
      "|        #biden| 2890|\n",
      "|#stopasianhate| 1238|\n",
      "|    #inflation|  657|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.groupBy(\"label\") \\\n",
    "      .count() \\\n",
    "      .orderBy(col(\"count\").desc()) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainDF, testDF) = tweets.randomSplit((0.80, 0.20), seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = StringIndexer(inputCol=\"label\", outputCol=\"target\")\n",
    "\n",
    "regexer = RegexTokenizer(inputCol='tweet_text', outputCol=\"tokens\", pattern=\"((https).+)|[^0-9a-z#+]+\", minTokenLength=3)\n",
    "\n",
    "stopworder = StopWordsRemover().setInputCol('tokens').setOutputCol('words')\n",
    "\n",
    "vectorizer = CountVectorizer(inputCol='words', outputCol=\"countFeatures\")\n",
    "\n",
    "idf = IDF(inputCol='countFeatures', outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol=\"target\", maxIter=20)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    labeler,\n",
    "    regexer,\n",
    "    stopworder,\n",
    "    vectorizer,\n",
    "    idf,\n",
    "    lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhile applying HashingTF only needs a single pass to the data, applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies by IDF.\\n\\nfrom pyspark.mllib.feature import IDF\\n\\n# ... continue from the previous example\\ntf.cache()\\nidf = IDF().fit(tf)\\ntfidf = idf.transform(tf)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "While applying HashingTF only needs a single pass to the data, applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "\n",
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "# ... continue from the previous example\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.2, 0.5, 0.8])\n",
    "  .addGrid(lr.regParam, [0.001, 0.01, 0.1])\n",
    "  .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "                          numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6322648944141701"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel = crossval.fit(trainDF)\n",
    "\n",
    "predictions = cvmodel.transform(testDF)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5741735864148919,\n",
       " 0.5839552718292101,\n",
       " 0.5968820516060527,\n",
       " 0.5944874251793408,\n",
       " 0.6228133092103406,\n",
       " 0.5485467018428257,\n",
       " 0.6060507405447884,\n",
       " 0.6287117845933049,\n",
       " 0.4266842552695711,\n",
       " 0.6115089812518024,\n",
       " 0.6204290134036783,\n",
       " 0.32292393344671866]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[3].getMinDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[5].getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[5].getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[5].getElasticNetParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_418448bd72c8, numClasses=6, numFeatures=26112"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.stages[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmodel.save('cvmodel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+----------+-----+\n",
      "|         label|target|prediction|count|\n",
      "+--------------+------+----------+-----+\n",
      "|      #vaccine|   0.0|       0.0|  537|\n",
      "|      #vaccine|   0.0|       1.0|  129|\n",
      "|      #vaccine|   0.0|       2.0|   33|\n",
      "|      #vaccine|   0.0|       3.0|   32|\n",
      "|      #vaccine|   0.0|       5.0|    1|\n",
      "|        #covid|   1.0|       0.0|  190|\n",
      "|        #covid|   1.0|       1.0|  405|\n",
      "|        #covid|   1.0|       2.0|   34|\n",
      "|        #covid|   1.0|       3.0|   47|\n",
      "|        #covid|   1.0|       4.0|    4|\n",
      "|        #covid|   1.0|       5.0|    1|\n",
      "|        #china|   2.0|       0.0|  103|\n",
      "|        #china|   2.0|       1.0|   88|\n",
      "|        #china|   2.0|       2.0|  384|\n",
      "|        #china|   2.0|       3.0|   54|\n",
      "|        #china|   2.0|       4.0|    5|\n",
      "|        #china|   2.0|       5.0|    4|\n",
      "|        #biden|   3.0|       0.0|   96|\n",
      "|        #biden|   3.0|       1.0|   74|\n",
      "|        #biden|   3.0|       2.0|   27|\n",
      "|        #biden|   3.0|       3.0|  360|\n",
      "|        #biden|   3.0|       4.0|   11|\n",
      "|        #biden|   3.0|       5.0|   10|\n",
      "|#stopasianhate|   4.0|       0.0|   40|\n",
      "|#stopasianhate|   4.0|       1.0|   27|\n",
      "|#stopasianhate|   4.0|       2.0|   10|\n",
      "|#stopasianhate|   4.0|       3.0|    8|\n",
      "|#stopasianhate|   4.0|       4.0|  143|\n",
      "|#stopasianhate|   4.0|       5.0|    1|\n",
      "|    #inflation|   5.0|       0.0|   29|\n",
      "|    #inflation|   5.0|       1.0|   19|\n",
      "|    #inflation|   5.0|       2.0|   19|\n",
      "|    #inflation|   5.0|       3.0|   16|\n",
      "|    #inflation|   5.0|       5.0|   63|\n",
      "+--------------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"label\", 'target', \"prediction\").count().sort('target', 'prediction').show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetToLabel = {\n",
    "    1.0: 'covid',\n",
    "    2.0: 'china',\n",
    "    3.0: 'biden',\n",
    "    4.0: 'stopasianhate',\n",
    "    5.0: 'inflation',\n",
    "    0.0: 'vaccine'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickled model via pipeline api\n",
    "#from pyspark.ml.pipeline import PipelineModel\n",
    "#persistedModel = PipelineModel.load(mPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
